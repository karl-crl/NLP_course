{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis.gensim\n",
    "import re\n",
    "from typing import List\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что еще можно сделать:\n",
    "1) Попробовать NMF\n",
    "2) Добавить n-граммы\n",
    "3) Optimize choice for number of topics through coherence measure\n",
    "4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrapping\n",
    "\n",
    "В качестве объекта скреппинга выбран ресурс PubMed с биологическими статьями. Вытаскивать со странички буду название и abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_from_id = lambda idx: \"https://pubmed.ncbi.nlm.nih.gov/\" + str(idx) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_page(idx: str):\n",
    "    page = requests.get(get_url_from_id(idx)).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    title = soup.title.text\n",
    "    \n",
    "    spans = soup.findAll('div')\n",
    "    abstract = None\n",
    "    classes = []\n",
    "    for span in spans:\n",
    "        try:\n",
    "            classes.extend(span['class'])\n",
    "            if 'abstract-content' in span['class']:\n",
    "                abstract = span\n",
    "                break\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    abstract = abstract.text\n",
    "    return title + ' ' + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_article = 29949996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = 0\n",
    "articles = []\n",
    "idx = start_article\n",
    "while cntr < 300:\n",
    "#     time.sleep(1)\n",
    "    try:\n",
    "        print(f\"Scrapping {idx}\")\n",
    "        txt = get_text_from_page(idx)\n",
    "        articles.append(txt)\n",
    "        cntr+=1\n",
    "        idx+=1\n",
    "    except Exception:\n",
    "        print(f\"Failed: {idx}\")\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrapped_data.pickle', 'wb') as f:\n",
    "    pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing\n",
    "\n",
    "### Plan:\n",
    "1. Tokenize\n",
    "2. Remove punctuation\n",
    "3. Hybride stemming\n",
    "4. Remmove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = articles.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [nltk.word_tokenize(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove puctuation tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation + \"``\" + \"\\'\\'\" + \"...\" + \"....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [list(filter(lambda token: token not in punc, text)) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chech whether all punctuation symbols removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    words.extend(text)\n",
    "words = list(set(words))\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-0.005',\n",
       " '-0.675',\n",
       " '-1-aminopropan-2-ol',\n",
       " '-10',\n",
       " '-13',\n",
       " '-2',\n",
       " '-25',\n",
       " '-4',\n",
       " '-7',\n",
       " '-Editorial']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что осталось много специфических символов, чисел, поскольку они часто встречаются в статьях, но не несут почти никакого смысла. Поэтому просто уберу их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\W\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\W\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\W\n",
      "<ipython-input-108-0249e7312b14>:4: DeprecationWarning: invalid escape sequence \\d\n",
      "  word = re.sub('\\d', \"\", wrd)\n",
      "<ipython-input-108-0249e7312b14>:5: DeprecationWarning: invalid escape sequence \\W\n",
      "  word = re.sub('\\W', \"\", word)\n"
     ]
    }
   ],
   "source": [
    "def filter_text_from_punct(txt: List[str]) -> List[str]:\n",
    "    result = []\n",
    "    for wrd in txt:\n",
    "        word = re.sub('\\d', \"\", wrd)\n",
    "        word = re.sub('\\W', \"\", word)\n",
    "        if (len(word) > 0):\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [filter_text_from_punct(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krovetz\n",
    "ks = krovetz.PyKrovetzStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[ks.stem(i) for i in text] for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[word for word in text if word not in stopwords] for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Drop short word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    words.extend(text)\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'c',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'n',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'z',\n",
       " 'Å',\n",
       " 'Φ',\n",
       " 'β',\n",
       " 'γ',\n",
       " 'κ',\n",
       " 'λ',\n",
       " '⁶',\n",
       " 'aa',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'ag',\n",
       " 'ah',\n",
       " 'ai']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(words, key=len)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно много одиночных букв (которые точно надо убрать) и слов длины два. Часть из слов длины 2 может быть важна, но среди них может быть и мусор, поэтому почищу их все."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_from_short(txt: List[str]) -> List[str]:\n",
    "    result = []\n",
    "    for wrd in txt:\n",
    "        if (len(wrd) > 2):\n",
    "            result.append(wrd)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [filter_text_from_short(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем словарь и векторизуем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  создаем словарь \n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "## 1. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "ldamodel = models.LdaModel(tfidf, id2word=dictionary, num_topics=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.001*\"health\" + 0.001*\"patient\" + 0.001*\"bone\" + 0.001*\"cell\"')\n",
      "(1, '0.001*\"patient\" + 0.001*\"cell\" + 0.001*\"expression\" + 0.001*\"women\"')\n",
      "(2, '0.001*\"patient\" + 0.001*\"gene\" + 0.001*\"group\" + 0.001*\"cancer\"')\n",
      "(3, '0.001*\"group\" + 0.001*\"patient\" + 0.001*\"cell\" + 0.001*\"expression\"')\n",
      "(4, '0.001*\"cell\" + 0.001*\"hiv\" + 0.001*\"ato\" + 0.001*\"health\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, tfidf):\n",
    "    m_score = 0\n",
    "    for index, score in sorted(model[tfidf[1]], key=lambda tup: -1*tup[1]):\n",
    "        if score > m_score:\n",
    "            m_score = score\n",
    "            m_topic = index\n",
    "    return m_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.001*\"patient\" + 0.001*\"group\" + 0.001*\"case\" + 0.001*\"ato\"'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_model = []\n",
    "lda.print_topic(get_topics(ldamodel, tfidf), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary=ldamodel.id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.231*\"patient\" + 0.194*\"cell\" + 0.174*\"expression\"'),\n",
       " (1, '0.289*\"health\" + -0.223*\"cell\" + -0.207*\"expression\"'),\n",
       " (2, '-0.263*\"patient\" + 0.197*\"cell\" + -0.156*\"aml\"'),\n",
       " (3, '-0.354*\"health\" + -0.241*\"mental\" + -0.160*\"cell\"'),\n",
       " (4, '0.289*\"decoction\" + 0.189*\"guizhi\" + 0.183*\"disease\"')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "lsi = models.LsiModel(tfidf, id2word=dictionary, num_topics=5)\n",
    "lsi.show_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7fa7ecd70950>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
